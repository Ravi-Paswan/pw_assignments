{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1b8dbc-dd4b-49bc-ab29-9f21547dc5e4",
   "metadata": {},
   "source": [
    "# Qo 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db6446-3daf-443f-827c-3ddfd2dd0c78",
   "metadata": {},
   "source": [
    "### What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6859def-22b8-479c-bf66-6ba0c693f428",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a popular data preprocessing technique used to transform numerical features into a common range. It rescales the values of a feature to fit within a specified range, typically between 0 and 1.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "scaled_value = (x - min_value) / (max_value - min_value)\n",
    "\n",
    "where x is the original value of the feature, min_value is the minimum value of the feature in the dataset, and max_value is the maximum value of the feature in the dataset.\n",
    "\n",
    "Min-Max scaling is particularly useful when the range of values in different features varies significantly. By scaling all features to the same range, it ensures that they have equal importance during modeling and prevents any particular feature from dominating the learning algorithm due to its larger value range.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose we have a dataset with a feature called \"Age\" that ranges from 20 to 60. We want to scale this feature using Min-Max scaling.\n",
    "\n",
    "Original Age values: [20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "\n",
    "To apply Min-Max scaling, we need to calculate the minimum and maximum values of the Age feature:\n",
    "\n",
    "min_value = 20\n",
    "max_value = 60\n",
    "\n",
    "Next, we use the formula to scale each value within the range of 0 to 1:\n",
    "\n",
    "scaled_value = (x - min_value) / (max_value - min_value)\n",
    "\n",
    "Scaled Age values: [0.0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0]\n",
    "\n",
    "As a result, the Age values are transformed into a common range between 0 and 1. This scaling ensures that all the age values have the same importance during the modeling process, regardless of their original range.\n",
    "\n",
    "Min-Max scaling can be applied to multiple features in a dataset to normalize the entire dataset. It is commonly used in machine learning algorithms, such as support vector machines (SVMs) and artificial neural networks, to improve their performance and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378385c-7f00-4477-beca-8f3c5bafa64b",
   "metadata": {},
   "source": [
    "# Qo 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f3f1e-5e8b-4120-87dd-7a798d2b64eb",
   "metadata": {},
   "source": [
    "### What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc57d04f-003a-4508-82c8-93e4813c410a",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization or vector normalization, is another data preprocessing technique used to scale numerical features. Unlike Min-Max scaling, which scales the values to a specific range, Unit Vector scaling transforms the values to have a magnitude of 1 while preserving their direction.\n",
    "\n",
    "The formula for Unit Vector scaling is as follows:\n",
    "\n",
    "scaled_value = x / ||x||\n",
    "\n",
    "where x is the original value of the feature, and ||x|| represents the Euclidean norm or magnitude of the feature vector.\n",
    "\n",
    "Unit Vector scaling is useful when the direction of the feature vectors is more important than their actual values. It ensures that all feature vectors have equal length, which can be beneficial in certain machine learning algorithms that rely on distances or similarity measures between vectors.\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Suppose we have a dataset with two numerical features, \"Height\" and \"Weight,\" and we want to apply Unit Vector scaling.\n",
    "\n",
    "Original Height values: [150, 160, 170, 180]\n",
    "Original Weight values: [50, 60, 70, 80]\n",
    "\n",
    "To apply Unit Vector scaling, we need to calculate the Euclidean norm or magnitude of each feature vector:\n",
    "\n",
    "||Height|| = sqrt(150^2 + 160^2 + 170^2 + 180^2)\n",
    "           = sqrt(22500 + 25600 + 28900 + 32400)\n",
    "           = sqrt(109400)\n",
    "           ≈ 330.63\n",
    "\n",
    "||Weight|| = sqrt(50^2 + 60^2 + 70^2 + 80^2)\n",
    "           = sqrt(2500 + 3600 + 4900 + 6400)\n",
    "           = sqrt(17400)\n",
    "           ≈ 131.95\n",
    "\n",
    "Next, we divide each feature value by its respective Euclidean norm:\n",
    "\n",
    "scaled_height = [150/330.63, 160/330.63, 170/330.63, 180/330.63]\n",
    "              ≈ [0.453, 0.484, 0.515, 0.546]\n",
    "\n",
    "scaled_weight = [50/131.95, 60/131.95, 70/131.95, 80/131.95]\n",
    "              ≈ [0.379, 0.455, 0.530, 0.606]\n",
    "\n",
    "As a result, the Height and Weight values are scaled such that their vectors have a magnitude of 1. This scaling preserves the direction of the feature vectors while eliminating the influence of their original magnitudes.\n",
    "\n",
    "Unit Vector scaling is commonly used in text classification, recommendation systems, and clustering algorithms. It allows for a meaningful comparison of vectors based on their directions and helps in cases where the absolute magnitudes of the features are not as important as their relative orientations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bd2e32-f841-4180-a04f-1b7d4bd6ab4d",
   "metadata": {},
   "source": [
    "# Qo 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26743bf-808b-4cc5-a662-dbd42e0af69a",
   "metadata": {},
   "source": [
    "### What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d98957-c01b-4da1-a674-6e1bad4d728a",
   "metadata": {},
   "source": [
    "PCA, which stands for Principal Component Analysis, is a statistical technique used for dimensionality reduction. It identifies the most important features, known as principal components, in a dataset and transforms the data into a new coordinate system based on these components. PCA is commonly used to simplify complex datasets with a high number of variables into a smaller set of variables while retaining most of the information.\n",
    "\n",
    "Here's an overview of how PCA works:\n",
    "\n",
    "1. Standardize the data: Before applying PCA, it is essential to standardize the data by subtracting the mean and dividing by the standard deviation. This step ensures that all variables have the same scale and prevents any single variable from dominating the analysis.\n",
    "\n",
    "2. Calculate the covariance matrix: The covariance matrix is computed based on the standardized data. It represents the relationships between variables, showing how they vary together.\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues: The eigenvectors and eigenvalues of the covariance matrix are calculated. Eigenvectors represent the principal components, and eigenvalues indicate the amount of variance explained by each component. The eigenvectors are sorted in descending order of their corresponding eigenvalues.\n",
    "\n",
    "4. Select the number of principal components: Based on the eigenvalues, the number of principal components to retain is determined. Typically, components with high eigenvalues are chosen to capture the most significant variation in the data.\n",
    "\n",
    "5. Transform the data: The original data is transformed into the new coordinate system defined by the selected principal components. This transformation yields a set of new variables, known as the principal component scores, which are linear combinations of the original variables.\n",
    "\n",
    "Here's an example to illustrate the application of PCA:\n",
    "\n",
    "Suppose we have a dataset with three variables: \"Height,\" \"Weight,\" and \"Age.\" We want to apply PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "Original data:\n",
    "Height: [150, 160, 170, 180]\n",
    "Weight: [50, 60, 70, 80]\n",
    "Age: [25, 30, 35, 40]\n",
    "\n",
    "1. Standardize the data: Subtract the mean and divide by the standard deviation for each variable.\n",
    "\n",
    "2. Calculate the covariance matrix:\n",
    "\n",
    "          Height   Weight   Age\n",
    "Height   1.67     1.67     0.33\n",
    "Weight   1.67     1.67     0.33\n",
    "Age      0.33     0.33     1.67\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues:\n",
    "\n",
    "Eigenvalues:\n",
    "[3.33, 0.0, 0.0]\n",
    "\n",
    "Eigenvectors:\n",
    "[0.707, -0.707, 0.0]\n",
    "[0.707, 0.707, 0.0]\n",
    "[0.0, 0.0, 1.0]\n",
    "\n",
    "4. Select the number of principal components: In this case, we choose to retain two principal components since they have non-zero eigenvalues and explain the most variance.\n",
    "\n",
    "5. Transform the data: Multiply the original data by the selected eigenvectors to obtain the principal component scores.\n",
    "\n",
    "Transformed data:\n",
    "PC1: [0.707*Height - 0.707*Weight]\n",
    "PC2: [0.707*Height + 0.707*Weight]\n",
    "\n",
    "The transformed data now consists of two principal components, PC1 and PC2, which are linear combinations of the original variables. These components capture most of the variance in the original data and provide a reduced-dimensional representation.\n",
    "\n",
    "PCA is widely used in various fields, including data analysis, pattern recognition, image processing, and feature extraction. It helps to uncover underlying patterns, reduce data complexity, and improve computational efficiency in machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae805b9-597a-4f48-bb5b-ac1d871ca43c",
   "metadata": {},
   "source": [
    "# Qo 04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5adf24-f5ea-414a-9d5b-ca7535127da1",
   "metadata": {},
   "source": [
    "### What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef14fe3-879b-4960-8e76-b988179ab8ac",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related concepts. In fact, PCA is a commonly used technique for feature extraction. Feature extraction refers to the process of transforming the original set of features into a new set of features that capture the most relevant information in the data while reducing its dimensionality.\n",
    "\n",
    "PCA achieves feature extraction by identifying the principal components of the data, which are linear combinations of the original features. These principal components are ordered based on the amount of variance they explain in the data. By selecting a subset of the principal components, PCA allows for the creation of a reduced-dimensional representation of the data that retains most of its essential information.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose we have a dataset with four numerical features: \"Height,\" \"Weight,\" \"Age,\" and \"Income.\" We want to extract the most informative features using PCA.\n",
    "\n",
    "Original data:\n",
    "Height: [150, 160, 170, 180]\n",
    "Weight: [50, 60, 70, 80]\n",
    "Age: [25, 30, 35, 40]\n",
    "Income: [25000, 35000, 45000, 55000]\n",
    "\n",
    "1. Standardize the data: Standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "2. Calculate the covariance matrix:\n",
    "\n",
    "          Height   Weight   Age      Income\n",
    "Height   1.00     1.00     0.50     0.70\n",
    "Weight   1.00     1.00     0.50     0.70\n",
    "Age      0.50     0.50     1.00     0.60\n",
    "Income   0.70     0.70     0.60     1.00\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues:\n",
    "\n",
    "Eigenvalues:\n",
    "[2.71, 0.69, 0.07, 0.03]\n",
    "\n",
    "Eigenvectors:\n",
    "[0.58, -0.58, -0.48, -0.34]\n",
    "[0.58, -0.58, 0.43, 0.38]\n",
    "[0.55, 0.55, -0.33, -0.53]\n",
    "[0.13, 0.13, 0.73, -0.66]\n",
    "\n",
    "4. Select the number of principal components: Based on the eigenvalues, we choose to retain the first two principal components since they have the highest eigenvalues and explain the most variance.\n",
    "\n",
    "5. Transform the data: Multiply the original data by the selected eigenvectors to obtain the principal component scores.\n",
    "\n",
    "Transformed data:\n",
    "PC1: [0.58*Height - 0.58*Weight - 0.48*Age - 0.34*Income]\n",
    "PC2: [0.58*Height - 0.58*Weight + 0.43*Age + 0.38*Income]\n",
    "\n",
    "The transformed data consists of two principal components, PC1 and PC2, which are linear combinations of the original features. These principal components capture most of the variance in the original data and provide a reduced-dimensional representation.\n",
    "\n",
    "In this example, PCA was used for feature extraction by identifying the two most informative features (PC1 and PC2) that capture the underlying patterns in the data. These principal components can be used as new features in subsequent analysis or modeling tasks, reducing the dimensionality of the data while preserving its essential characteristics.\n",
    "\n",
    "Feature extraction with PCA can be particularly useful in scenarios where the original feature space is high-dimensional, noisy, or redundant. It helps to reduce the computational complexity of subsequent tasks, enhance interpretability, and improve the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e93702-69ac-46df-87e8-9be7f59d1836",
   "metadata": {},
   "source": [
    "# Qo 05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ec9da-5fae-4b4a-878f-059fa45a107b",
   "metadata": {},
   "source": [
    "### You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ac964-bbfb-41cc-bd67-7ba08d47ad60",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service, you can use Min-Max scaling to normalize the features such as price, rating, and delivery time. Here's how you can apply Min-Max scaling:\n",
    "\n",
    "1. Understand the range of each feature: Examine the minimum and maximum values of each feature (price, rating, and delivery time) in the dataset.\n",
    "\n",
    "2. Apply Min-Max scaling: For each feature, use the Min-Max scaling formula to transform the values into a common range between 0 and 1:\n",
    "\n",
    "   scaled_value = (x - min_value) / (max_value - min_value)\n",
    "\n",
    "   where x represents the original value of the feature, min_value is the minimum value of the feature in the dataset, and max_value is the maximum value of the feature in the dataset.\n",
    "\n",
    "3. Normalize each feature: Apply the Min-Max scaling formula to normalize the values of each feature individually. This ensures that all the features are on the same scale and prevents any particular feature from dominating the recommendation algorithm due to its larger value range.\n",
    "\n",
    "4. Update the dataset: Replace the original values of each feature with their corresponding scaled values after applying Min-Max scaling.\n",
    "\n",
    "By using Min-Max scaling, you will transform the features such as price, rating, and delivery time into a common range between 0 and 1. This scaling ensures that all the features have equal importance during the recommendation process, regardless of their original value ranges.\n",
    "\n",
    "For example, let's say you have the following values for each feature in the dataset:\n",
    "\n",
    "Price: [10, 20, 30, 40]\n",
    "Rating: [2.5, 3.7, 4.2, 4.8]\n",
    "Delivery Time: [20, 30, 40, 50]\n",
    "\n",
    "To apply Min-Max scaling, you calculate the minimum and maximum values for each feature:\n",
    "\n",
    "Price: min_value = 10, max_value = 40\n",
    "Rating: min_value = 2.5, max_value = 4.8\n",
    "Delivery Time: min_value = 20, max_value = 50\n",
    "\n",
    "Then, you use the Min-Max scaling formula to normalize the values of each feature:\n",
    "\n",
    "Scaled Price: [(10-10)/(40-10), (20-10)/(40-10), (30-10)/(40-10), (40-10)/(40-10)]\n",
    "            = [0, 0.333, 0.667, 1]\n",
    "\n",
    "Scaled Rating: [(2.5-2.5)/(4.8-2.5), (3.7-2.5)/(4.8-2.5), (4.2-2.5)/(4.8-2.5), (4.8-2.5)/(4.8-2.5)]\n",
    "             = [0, 0.509, 0.769, 1]\n",
    "\n",
    "Scaled Delivery Time: [(20-20)/(50-20), (30-20)/(50-20), (40-20)/(50-20), (50-20)/(50-20)]\n",
    "                    = [0, 0.333, 0.667, 1]\n",
    "\n",
    "After applying Min-Max scaling, the dataset will have the normalized values for each feature, which can be used for building the recommendation system. The scaled values ensure that all the features have equal importance, and the algorithm can effectively consider their relative magnitudes when making recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5ba96-7629-4d5f-ab12-52dd3dc9fcc5",
   "metadata": {},
   "source": [
    "# Qo 06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80baea10-3372-44bb-b0e3-c8def478869b",
   "metadata": {},
   "source": [
    "### You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969f891-f653-4692-a9e4-6d6381d37ff3",
   "metadata": {},
   "source": [
    "When building a model to predict stock prices with a dataset containing numerous features, PCA can be employed to reduce the dimensionality and extract the most important information. Here's how you can use PCA for dimensionality reduction in this context:\n",
    "\n",
    "1. Data preparation: Preprocess the dataset by standardizing the features. Subtract the mean and divide by the standard deviation for each feature to ensure they are on a similar scale.\n",
    "\n",
    "2. Covariance matrix: Calculate the covariance matrix using the standardized dataset. The covariance matrix represents the relationships between the different features, indicating how they vary together.\n",
    "\n",
    "3. Eigenvalue decomposition: Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues. Eigenvectors represent the principal components, and eigenvalues denote the amount of variance explained by each component.\n",
    "\n",
    "4. Sorting eigenvalues: Sort the eigenvalues in descending order. This ranking reflects the significance of each principal component in capturing the variance in the dataset.\n",
    "\n",
    "5. Selecting principal components: Determine the number of principal components to retain based on the cumulative explained variance. A common approach is to choose the number of components that capture a significant portion of the total variance, such as 80% or 90%.\n",
    "\n",
    "6. Projection: Project the original dataset onto the selected principal components to obtain the reduced-dimensional representation. Multiply the standardized dataset by the eigenvectors corresponding to the retained principal components.\n",
    "\n",
    "By following these steps, you can reduce the dimensionality of the dataset while preserving most of the information. The retained principal components capture the underlying patterns and variability in the data, allowing you to work with a smaller set of features in the subsequent modeling process.\n",
    "\n",
    "Dimensionality reduction with PCA is particularly useful when dealing with datasets that have a large number of features, as it helps to eliminate redundant or less informative variables. By focusing on the principal components that explain the most variance, you can simplify the dataset and potentially improve the model's performance, reduce overfitting, and enhance interpretability.\n",
    "\n",
    "After applying PCA and obtaining the reduced-dimensional representation of the dataset, you can proceed to train a prediction model using the transformed features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9714fb2-9c68-4280-8eeb-2f61b6fb0206",
   "metadata": {},
   "source": [
    "# Qo 07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204a4ee-6726-43f8-af24-f098a7172e58",
   "metadata": {},
   "source": [
    "### For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "793e9080-e430-48ad-ae24-a0771fcffd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([1,5,10,15],columns = [\"values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0cbce9-9055-457f-b4e7-643d704195f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000000\n",
       "1    0.285714\n",
       "2    0.642857\n",
       "3    1.000000\n",
       "Name: values, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"values\"].apply(lambda x:(x-min(df[\"values\"]))/(max(df[\"values\"])-min(df[\"values\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2b365-26f3-403c-8bac-8b95d4ee35ee",
   "metadata": {},
   "source": [
    "# Qo 08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17278ab-7b80-440f-acc0-db32e9fbfb4e",
   "metadata": {},
   "source": [
    "### For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde2b5c-051c-4f43-99bd-cc8c11bca38e",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on the given dataset with features [height, weight, age, gender, blood pressure], we can follow these steps:\n",
    "\n",
    "1. Preprocess the data: Standardize the numerical features (height, weight, age, blood pressure) by subtracting the mean and dividing by the standard deviation. Categorical features like gender may need to be encoded as numeric values.\n",
    "\n",
    "2. Compute the covariance matrix: Calculate the covariance matrix based on the standardized data. The covariance matrix represents the relationships between the features and provides insights into their variations.\n",
    "\n",
    "3. Compute eigenvectors and eigenvalues: Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. Sort eigenvalues: Sort the eigenvalues in descending order. This step helps determine the most significant principal components that explain the most variance in the data.\n",
    "\n",
    "5. Retain principal components: Decide on the number of principal components to retain based on the cumulative explained variance. Retaining a subset of the principal components that explain a significant portion of the total variance allows for dimensionality reduction while retaining most of the information.\n",
    "\n",
    "The number of principal components to retain can be determined by considering the cumulative explained variance. The cumulative explained variance shows the proportion of total variance explained by each principal component when summed up in order of their eigenvalues. It helps in assessing how much variance is retained by selecting a certain number of principal components.\n",
    "\n",
    "To determine the number of principal components to retain, you can examine the cumulative explained variance plot and choose a threshold. A commonly used threshold is to retain principal components that explain a cumulative variance of 80% or 90%.\n",
    "\n",
    "The decision on the number of principal components to retain may also depend on the specific requirements of the problem and the trade-off between dimensionality reduction and information retention.\n",
    "\n",
    "Without the actual data or specific details about the dataset, it is challenging to provide an exact number of principal components to retain. However, you can follow the steps outlined above, compute the cumulative explained variance, and choose the number of principal components that retain a satisfactory amount of variance based on the specific problem and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c292534-7075-4489-bbad-d37a989a0266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
